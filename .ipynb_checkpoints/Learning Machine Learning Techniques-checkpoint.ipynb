{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Datasets\n",
    "\n",
    "Where shall I find datasets to prectice Machine Learning Techniques?\n",
    "\n",
    "The following data repositories are useful in finding data for the above purpose:\n",
    "\n",
    "1. [Kaggle](https://www.kaggle.com/datasets) - Popular platform for data science competitionns that provides a wide range of datasets.\n",
    "2. [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/) - A repository that hosts various datasets that are commonly used in machine learning research.\n",
    "3. [GitHub](https://github.com/) - Many researchers and organizations publish their datasets on GitHub so it is worth exploring\n",
    "4. [OpenML](https://www.openml.org/) - This is an online platform that hosts a large collection of datasets for machine learning research. It also provides an API and a web interface to access and download the datasets\n",
    "\n",
    "**NOTE**\n",
    "- It is important to carefully review the terms of use and licensing agreements for any dataset you intend to use. \n",
    "- It is also importnat to ensure that you have the necessary permissions to use the data for training your machine learning models, especially when it comes to sensitive topics involving personal information such as medical and financial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary 25 Steps to building/practice ML Techniques\n",
    "\n",
    "The following is a template we can follow to understand some techniques and algorithms in machine learning; it is not exhaustive so it can be adjusted as deemed necessary.\n",
    "\n",
    "1. Load the datset into a Pandas DataFrame\n",
    "2. Explore the dataset by displaying the first few rows and checking the datatypes of each column\n",
    "3. Check if there are any missing values in the datset and handle them appropriately (e.g through imputation or removal)\n",
    "4. Split the dataset into **features** (input variable) and **lables** (e.g. fraud or non-fraud)\n",
    "5. Perform feature scaling on the input variable to ensure that they are on a similar scale\n",
    "6. Split the data into training and testing sets using a ratio of choice e.g. 70:30\n",
    "7. Train the model of choice (Logistic Regression) on the training set and evaluate its performance on the testing set\n",
    "8. Train a another model e.g a random forest classifier on the training set and evaluate its performance on the testing set\n",
    "9. Implement k-fold cross-validation with 5 folds on the Logistic regression model and compute the average accuracy\n",
    "10. Use grid search to find the best hyperparameters for the random forest classifier (e.g., number if trees, maximum depth).\n",
    "11. Train the random forest classifier with the best hyperparameters found in the previous step and evaluate its performance on the testing set\n",
    "12. Compute the precision, recall and F1-score of the random forest classifier on the testing set\n",
    "13. Visualize the feature importances using the random forest classifier using a bar chart\n",
    "14. Implement oversampling (e.g., SMOTE) to address class imbalance in the dataset and retain the random forest classifier\n",
    "15. Compare the performance of the original and oversampled models using a precision-recall curve\n",
    "16. Train a support vector machine (SVM) classifier on the training set and evaluate its performance on the testing set\n",
    "17. Compute the area under the receiver operating characteristic curve (AUC-ROC) for the SVM classifier\n",
    "18. Use the ADASYN algorithm to generate syntheti samples for the minority class and retain the SVM classifier\n",
    "19. Compare the performance of the original and ADASYN models using a ROC curve\n",
    "20. Implement decision tree classifier and evaluate its performance on the testing set\n",
    "21. Perform feature selection usinf recursive feature elimination with cross-validation (RFECV) and retrain the decision tree classifier\n",
    "22. Compare the performance of the original and feature-selected models using confusion matrix\n",
    "23. Use the XGBoost algorithm to train a boosted tree classifier on the training set and evaluate its performance on the testing set\n",
    "24. Tune the hyperparameters of XGBoost classifier using random search or grid search\n",
    "25. Compare the performance of the original and tuned models using a precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
